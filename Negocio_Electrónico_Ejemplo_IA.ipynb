{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdución a Tensorflow y Keras\n",
        "\n",
        "Referencias:\n",
        "\n",
        "- [Introducción a TensorFlow y Keras: Fundamentos y ejemplos](https://openwebinars.net/blog/tensorflow-keras-fundamentos/). OpenWebinars.\n",
        "- [Introducción a las redes de memoria a corto-largo plazo (LSTM)](https://la.mathworks.com/discovery/lstm.html). MathWorks.\n",
        "- [What Is a Recurrent Neural Network (RNN)?](https://la.mathworks.com/discovery/rnn.html). MathWorks.\n",
        "\n"
      ],
      "metadata": {
        "id": "z_gNQNMCjmaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalación de Tensorflow 2\n",
        "\n",
        "- Necesitará un versión de Python que esté entre la **3.8 y [3.11](https://www.python.org/downloads/release/python-3119/)**.\n",
        "- Cuando trabaje en su equipo de forma local se recomienda el uso [**entornos virtuales** con `virtualenv`](https://josejuansanchez.org/python-for-java-developers/#_entornos_virtuales).\n",
        "- Descarga e instala el paquete de `tensorflow` con `pip`.\n",
        "\n"
      ],
      "metadata": {
        "id": "f_-dtsDGkawi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpMHngFF6qhe",
        "outputId": "3215e859-4bcc-4bca-aab9-1a7a3273aca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHcq9JQS64AI"
      },
      "source": [
        "## 2. Ejemplo de creación de un modelo LSTM\n",
        "\n",
        "En este ejemplo vamos a construir un modelo **LSTM (_Long Short-Term Memory_)** para un Chatbot que será capaz de analizar una frase de un cliente y clasificarla en uno de las siguientes categorías:\n",
        "\n",
        "- AYUDA\n",
        "- OK\n",
        "- SERVICIO_TECNICO\n",
        "\n",
        "\n",
        "Los pasos que vamos a seguir son:\n",
        "\n",
        "- Cargar un _dataset_ con los datos de entrenamiento. En nuestro caso será un archivo de texto.\n",
        "- Construir un modelo de una red reuronal de tipo LSTM capaz de analizar texto.\n",
        "- Entrenar la red neuronal.\n",
        "- Evaluar la precisión del modelo.\n",
        "\n",
        "### 2.1 Datos de entrenamiento\n",
        "\n",
        "El archivo de texto que vamos a utilizar contiene frases con el siguiente formato:\n",
        "\n",
        "```\n",
        "Frase : CATEGORÍA\n",
        "```\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "\n",
        "```\n",
        "No tengo idea de qué hacer, necesito orientación : AYUDA\n",
        "El equipo está funcionando sin problemas : OK\n",
        "Necesito que un profesional venga a mi casa para inspeccionar el equipo : SERVICIO_TECNICO\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5sN2dmH6xb3",
        "outputId": "d56698f4-a304-4563-95f3-85ca8c0159ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.6330 - loss: 0.9493\n",
            "Epoch 2/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.1393\n",
            "Epoch 3/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9896 - loss: 0.0501\n",
            "Epoch 4/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0192\n",
            "Epoch 5/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0109\n",
            "Epoch 6/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0071\n",
            "Epoch 7/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0045\n",
            "Epoch 8/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0032\n",
            "Epoch 9/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0022\n",
            "Epoch 10/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0016\n",
            "Epoch 11/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0011\n",
            "Epoch 12/12\n",
            "\u001b[1m570/570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.8378e-04\n",
            "Chatbot: ¡Hola! Estoy listo para responder tus preguntas. Escribe 'salir' para finalizar.\n",
            "Tú: No sé cómo arreglar esto\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step\n",
            "[[5.5222877e-04 9.9928516e-01 1.6265229e-04]]\n",
            "1\n",
            "Chatbot: AYUDA\n",
            "¿Correcto? (S/N)s\n",
            "Tú: salir\n"
          ]
        }
      ],
      "source": [
        "# Importamos Tensorflow y NumPy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\"\"\"\n",
        "Función para cargar los datos de entrenamiento.\n",
        "Esta función lee los datos de entrenamiento desde un archivo de texto\n",
        "los almacena en dos listas: preguntas y respuestas, que luego devuevle\n",
        "\"\"\"\n",
        "def cargar_datos(nombre_archivo):\n",
        "  preguntas = []\n",
        "  respuestas = []\n",
        "  with open(nombre_archivo, 'r') as file:\n",
        "    #i = 1\n",
        "    for line in file:\n",
        "      pregunta, respuesta = line.strip().split(' : ')\n",
        "      preguntas.append(pregunta)\n",
        "      respuestas.append(respuesta)\n",
        "      #print(f\"{i} - {line}\")\n",
        "      #i += 1\n",
        "  return preguntas, respuestas\n",
        "\n",
        "\"\"\"\n",
        "Función para preprocesar los datos.\n",
        "Convierte las palabras en símbolos, para poder trabajar con ellas.\n",
        "\"\"\"\n",
        "def preprocesar_datos(preguntas, respuestas):\n",
        "    # Creamos una capa de TextVectorization de TensorFlow, que nos ayuda\n",
        "    # a convertir texto en representaciones numéricas (vectores o tokens).\n",
        "    tokenizer = tf.keras.layers.TextVectorization()\n",
        "\n",
        "    # El tokenizer \"aprende\" el vocabulario que se usa en las preguntas,\n",
        "    # analiza su frecuencia y crea un mapeo único de palabras/tokens a índices numéricos.\n",
        "    tokenizer.adapt(preguntas)\n",
        "\n",
        "    # Las preguntas se convierten en secuencias numéricas usando el vocabulario aprendido.\n",
        "    # Por ejemplo: \"¿Cómo estás?\" podría transformarse en [3, 15].\n",
        "    x_train = tokenizer(preguntas)\n",
        "\n",
        "    # Las respuestas se convierten en un array de NumPy, que es necesario\n",
        "    # para la compatibilidad con TensorFlow/Keras durante el entrenamiento.\n",
        "    y_train = np.array(respuestas)\n",
        "\n",
        "    return x_train, y_train, tokenizer\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Función para construir el modelo LSTM (Long Short-Term Memory).\n",
        "Este modelo está diseñado para procesar texto y clasificarlo en categorías.\n",
        "\n",
        "El modelo es una red secuencial (Sequential) con tres capas clave:\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(...),  # Capa 1: Embedding\n",
        "    tf.keras.layers.LSTM(4),         # Capa 2: LSTM\n",
        "    tf.keras.layers.Dense(3, ...)    # Capa 3: Salida\n",
        "])\n",
        "\n",
        "- Capa 1:\n",
        "Convierte tokens numéricos (ej: [1, 14, 3]) en vectores densos de tamaño fijo.\n",
        "\n",
        "- Capa 2:\n",
        "Procesa secuencias de embeddings para capturar dependencias temporales como contexto en texto).\n",
        "El número de unidades/células LSTM determina la capacidad de aprendizaje del modelo.\n",
        "En este ejemplo se han utilizado 4 unidades/células.\n",
        "Aumentar este valor (Ejemplo: 64) puede mejorar la capacidad del modelo,\n",
        "pero requiere más datos para evitar overfitting.\n",
        "\n",
        "- Capa 3:\n",
        "Clasifica la secuencia en una de las 3 clases (definidas por num_clases).\n",
        "\"\"\"\n",
        "def construir_modelo(tokenizer, num_clases):\n",
        "  # El modelo es una red secuencial (Sequential) con tres capas.\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(input_dim=len(tokenizer.get_vocabulary()) + 1, output_dim=4,  mask_zero=True),\n",
        "      tf.keras.layers.LSTM(4), # La modificación de este parámetro redunda en la \"inteligencia\" del modelo\n",
        "      tf.keras.layers.Dense(num_clases, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  # Compilación del modelo.\n",
        "  # Elegimos una función de pérdida (loss): sparse_categorical_crossentropy (adecuada para clases enteras, como 0, 1, 2).\n",
        "  # Elegimos un optimizador (optimizer): adam (eficiente y popular para entrenamiento).\n",
        "  # Métrica: accuracy (porcentaje de predicciones correctas).\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\"\"\"\n",
        "Función para entrenar el modelo.\n",
        "- epoch: Es un ciclo completo de entrenamiento, donde el modelo ve todas las muestras del dataset una vez.\n",
        "  El número de epochs se determina mediante validación (Ej: cuando la pérdida en la validación deja de mejorar).\n",
        "\n",
        "- batch_size: Define cuántas muestras se procesan antes de actualizar los pesos del modelo.\n",
        "  Podemos decir que controla lo fino del entrenamiento.\n",
        "\n",
        "  Los valores de epoch y batch_size son responsables de que el entrenamiento tarde más o menos.\n",
        "\n",
        "- verbose: Es la cantidad de información que muestra durante el entrenamiento\n",
        "\"\"\"\n",
        "def entrenar_modelo(model, x_train, y_train):\n",
        "  model.fit(x_train, y_train, epochs=12, batch_size=1, verbose=True)\n",
        "\n",
        "\"\"\"\n",
        "Función para probar el chatbot\n",
        "\"\"\"\n",
        "def probar_chatbot(model, tokenizer, etiquetas):\n",
        "  print(\"Chatbot: ¡Hola! Estoy listo para responder tus preguntas. Escribe 'salir' para finalizar.\")\n",
        "\n",
        "  while True:\n",
        "    # Leemos una pregunta por teclado\n",
        "    preguntab = input(\"Tú: \")\n",
        "    if preguntab.lower() == \"salir\":\n",
        "      break\n",
        "\n",
        "    # Convierte la pregunta en una secuencia numérica (vector/token)\n",
        "    pregunta = tokenizer([preguntab])\n",
        "\n",
        "    # El modelo predice la categoría de la pregunta\n",
        "    respuesta = model.predict(pregunta)\n",
        "    print(respuesta)\n",
        "\n",
        "    # Obtenemos el índice de la clase con mayor probabilidad según la predicción del modelo.\n",
        "    indice = np.argmax(respuesta)\n",
        "    print(indice)\n",
        "\n",
        "    # Obtenemos el valor del índice. Necesitamos invertir el diccionario de etiquetas\n",
        "    # Origen:  etiquetas = {\"OK\": 0, \"AYUDA\": 1, \"SERVICIO_TECNICO\": 2}\n",
        "    # Destino: etiquetas_inversas = {0: \"OK\", 1: \"AYUDA\", 2: \"SERVICIO_TECNICO\"}\n",
        "    etiquetas_inversas = {valor: clave for clave, valor in etiquetas.items()}\n",
        "    categoria = etiquetas_inversas.get(indice, \"NO ENTIENDO\")\n",
        "\n",
        "    # Esto se usa para añadir más muestras de entrenamiento al archivo y,\n",
        "    # para que el modelo pueda aprender de ellas en un entrenamiento posterior.\n",
        "    print(\"Chatbot: \" + categoria)\n",
        "\n",
        "    valido = input(\"¿Correcto? (S/N)\")\n",
        "\n",
        "    if valido.lower() == \"s\":\n",
        "      # Nombre del archivo de texto donde están los datos de entrenamiento\n",
        "      nombre_archivo = \"tsetdesordenado.txt\"\n",
        "\n",
        "      # Abre el archivo en modo de escritura (append) para añadir contenido al final\n",
        "      with open(nombre_archivo, \"a\") as archivo:\n",
        "          # Escribe PREGUNTA, \" : \", y CATEGORIA al final del archivo\n",
        "          archivo.write(f\"\\n{preguntab} : {categoria}\")\n",
        "\n",
        "\"\"\"\n",
        "Función para convertir las etiquetas de las categorías en índices.\n",
        "Entrada:   etiquetas = {\"OK\": 0, \"AYUDA\": 1, \"SERVICIO_TECNICO\": 2}\n",
        "           y_train = [\"SERVICIO_TECNICO\", \"OK\", \"AYUDA\", \"OK\", \"AYDUDA\", ...]\n",
        "Salida:    y = [2, 0, 1, 0, 1, ...]\n",
        "\"\"\"\n",
        "def mapear_etiquetas_a_indices(y_train, etiquetas):\n",
        "    # Convertimos las etiquetas de las categorías en índices\n",
        "    y = [etiquetas[label] for label in y_train]\n",
        "\n",
        "    # Convertimos la lista de enteros en un tensor de TensorFlow\n",
        "    y = tf.convert_to_tensor(y, dtype=tf.int64)\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "# Programa Principal\n",
        "if __name__ == \"__main__\":\n",
        "    # Paso 1. Cargamos los datos de entrenamiento\n",
        "    archivo_entrenamiento = \"tsetdesordenado.txt\"\n",
        "    preguntas, respuestas = cargar_datos(archivo_entrenamiento)\n",
        "    x_train, y_train, tokenizer = preprocesar_datos(preguntas, respuestas)\n",
        "\n",
        "    # Convertimos el texto de las categorías de la lista y_train en índices.\n",
        "    etiquetas = {\"OK\": 0, \"AYUDA\": 1, \"SERVICIO_TECNICO\": 2}\n",
        "    y = mapear_etiquetas_a_indices(y_train, etiquetas)\n",
        "\n",
        "    # Convertimos la lista de respuestas en un set para eliminar los elementos repetidos\n",
        "    # Una vez que eliminamos los elementos repetidos contamos cuántos tipos hay.\n",
        "    num_clases = len(set(respuestas))\n",
        "\n",
        "    # Paso 2. Construimos el modelo\n",
        "    model = construir_modelo(tokenizer, num_clases)\n",
        "\n",
        "    # Paso 3. Entrenamos el modelo\n",
        "    entrenar_modelo(model,  x_train, y)\n",
        "\n",
        "    # Paso 4. Evaluamos el modelo\n",
        "    probar_chatbot(model, tokenizer, etiquetas)\n",
        "\n",
        "    # ----- Esto es una prueba -----\n",
        "    # Guardar el modelo entrenado en formato Keras (o .h5)\n",
        "    model.save(\"modelo.keras\")\n",
        "\n",
        "    # Guardar el vocabulario del tokenizer\n",
        "    vocabulary = tokenizer.get_vocabulary()\n",
        "    with open(\"vocabulary.txt\", \"w\") as f:\n",
        "        for word in vocabulary:\n",
        "            f.write(f\"{word}\\n\")\n",
        "\n",
        "    # Guardar el mapeo de etiquetas en un archivo\n",
        "    with open(\"etiquetas.json\", \"w\") as f:\n",
        "        json.dump(etiquetas, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo de cómo utilizar un modelo ya entrenado"
      ],
      "metadata": {
        "id": "3gUFpdMu9fxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# 1. Cargar el modelo\n",
        "model = tf.keras.models.load_model(\"modelo.keras\")\n",
        "\n",
        "# 2. Cargar el vocabulario del tokenizer\n",
        "with open(\"vocabulary.txt\", \"r\") as f:\n",
        "    vocabulary = [line.strip() for line in f]\n",
        "\n",
        "# 3. Crear el tokenizer y asignar el vocabulario\n",
        "tokenizer = tf.keras.layers.TextVectorization()\n",
        "tokenizer.set_vocabulary(vocabulary)\n",
        "\n",
        "# 4. Cargar el mapeo de etiquetas\n",
        "with open(\"etiquetas.json\", \"r\") as f:\n",
        "    etiquetas = json.load(f)\n",
        "    # Invertir el mapeo (de índice a etiqueta)\n",
        "    index_to_label = {v: k for k, v in etiquetas.items()}\n",
        "\n",
        "# Función para preprocesar texto de entrada\n",
        "def preprocesar_texto(texto, tokenizer):\n",
        "    texto_preprocesado = tokenizer([texto])\n",
        "    return texto_preprocesado\n",
        "\n",
        "# Bucle interactivo\n",
        "print(\"Chatbot: ¡Hola! Escribe 'salir' para terminar.\")\n",
        "while True:\n",
        "    entrada = input(\"Tú: \")\n",
        "    if entrada.lower() == \"salir\":\n",
        "        break\n",
        "\n",
        "    # Preprocesar y predecir\n",
        "    texto_preprocesado = preprocesar_texto(entrada, tokenizer)\n",
        "    prediccion = model.predict(texto_preprocesado)\n",
        "    indice = np.argmax(prediccion[0])  # Obtener el índice de mayor probabilidad\n",
        "\n",
        "    # Obtener la etiqueta correspondiente\n",
        "    etiqueta = index_to_label.get(indice, \"NO ENTIENDO\")\n",
        "    print(f\"Chatbot: {etiqueta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5RfYSez6Lzy",
        "outputId": "5cbf3e5b-5fd4-4036-b418-02c52391fe32"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot: ¡Hola! Escribe 'salir' para terminar.\n",
            "Tú: Todo está funcionando bien\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step\n",
            "Chatbot: OK\n",
            "Tú: Me gustaría hablar con alguien\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
            "Chatbot: AYUDA\n",
            "Tú: Necesito que alguien venga a mi domicilio\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Chatbot: SERVICIO_TECNICO\n",
            "Tú: salir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función auxiliar\n",
        "\n",
        "Esta función nos ayuda a desordenar las líneas del archivo de datos de entrenamiento."
      ],
      "metadata": {
        "id": "olOWeRAsz188"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def barajar_lineas_archivo(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Baraja las líneas de un archivo de entrada y las guarda en un archivo de salida.\n",
        "\n",
        "    Parámetros:\n",
        "    input_file (str): Ruta del archivo de entrada\n",
        "    output_file (str): Ruta del archivo de salida\n",
        "    \"\"\"\n",
        "    # Leer todas las líneas del archivo de entrada\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Barajar las líneas\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    # Escribir las líneas barajadas en el archivo de salida\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(lines)"
      ],
      "metadata": {
        "id": "ZoaiUIBEz0K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barajar_lineas_archivo(\"tset.txt\", \"tsetdesordenado.txt\")"
      ],
      "metadata": {
        "id": "7OFNTnV90C0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOsybZr817zt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}